---
title: "Dynamic stimulus representations in adapting neuronal networks"
event: "Human Brain Project (HBP) Workshop: *Stochastic Neural Computation*"
event_url: https://www.eitn.org/index.php/conferences-modalities/event-records

location: European Institute for Theoretical Neuroscience (EITN), Paris, France
address:
  street: ""
  city: ""
  region: ""
  postcode: ""
  country: ""

summary: ""
abstract: "In this work, we numerically explore the quality and spatiotemporal characteristics of dynamic stimulus representations in inhibition dominated, sparsely coupled recurrent networks of IF neurons, as well as the role played by ongoing, background activity in active processing and computation. We assume that the characteristics of stimulus representations are highly dependent on the current state of the circuit upon stimulus arrival and thus necessarily bound to the properties of ongoing, background activity.
Our networks are additionally endowed with a combination of timing-dependent synaptic plasticity mechanisms which provide an ongoing modulation of the balance of E/I. 
We begin by assessing the impact of plasticity on ongoing activity, demonstrating that it greatly increases the robustness of asynchronous irregular states, which are characterized by Poison-like population activity. In the presence of plasticity, this pattern is observed in a much broader  parameter range when compared to networks whose synapses are fixed and static. To perform this analysis, we systematically vary two control parameters: the relative strengths of E/I and the rate of a constant, unspecific and stochastic input). Plasticity is also shown to completely abolish states of synchronous regular population activity. We additionally demonstrate that these pathological states greatly reduced the network's capacity for online processing of time-varying input streams (or the network's kernel quality). On the opposite end, the stochastic AI states are related to a greater computational capacity. By increasing the robustness of this activity profile, the combined action of these plasticity mechanisms is shown to improve generic computational capacity.
Subsequently, the networks are stimulated with a series of topographically mapped input pulses, formalized as  a series of independent inhomogeneous Poisson processes whose rates are determined by these impulse kernels and whose activity is delivered to specifically tunned sub-populations within the main network.
Under biologically plausible input conditions, the action of plasticity is demonstrated to improve robustness and reproducibility of the emergent stimulus-specific spatiotemporal response transients, allowing a population of linear readouts to discriminate the identity of the stimuli throughout the entire time course of the observed responses.
This effect is shown to be mainly justified by the decorrelating effects of iSTDP, which actively maintains the AI activity profile both for ongoing activity and stimulus-driven responses. This stochastic firing pattern allows the network to more efficiently explore its state-space, and lies in opposition to the increasingly redundant and restrictive dynamical state observed in static networks, where states of greater synchrony and regularity come to dominate the responses."

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: "2014-11-27"
date_end: ""
all_day: true

# Schedule page publish date (NOT talk date).
publishDate: "2018-05-05T00:00:00Z"

authors: 
  - admin
tags: []

# Is this a featured talk? (true/false)
featured: false

image:
  caption: ""
  focal_point: Right

links:
- icon: object-group
  icon_pack: fas
  name: Poster
  url: "https://doi.org/10.6084/m9.figshare.11698626.v1"
  
# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []

# Enable math on this page?
math: true
---

